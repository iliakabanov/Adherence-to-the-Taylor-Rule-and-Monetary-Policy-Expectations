{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/iliakabanov/Adherence-to-the-Taylor-Rule-and-Monetary-Policy-Expectations/blob/main/works_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Устанавливаем окружение"
      ],
      "metadata": {
        "id": "1FkeSh60NO42"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "abgfwSe8SGua"
      },
      "outputs": [],
      "source": [
        "# !pip cache purge\n",
        "# !pip uninstall -y numpy scipy rectools\n",
        "# !pip install \"numpy<1.29.0\" \"scipy<1.11.0\" rectools\n",
        "# !pip install lightning_fabric\n",
        "# !pip install rectools\n",
        "# !pip install rectools[torch]\n",
        "# !pip install gensim\n",
        "# !pip install -U sentence-transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Имортируем библиотеки и запускаем spark-сессию"
      ],
      "metadata": {
        "id": "aq6XmgtRNWIo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hTg0Os8HMQI_",
        "outputId": "971603a0-e2e4-4fc9-ddc0-12762c74bc39"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ],
      "source": [
        "# Get access to Google disk\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bSRv58o8YFiY"
      },
      "outputs": [],
      "source": [
        "# Standard library imports\n",
        "import os\n",
        "import re\n",
        "import pickle\n",
        "import warnings\n",
        "import textwrap\n",
        "from pathlib import Path\n",
        "from collections import Counter\n",
        "from itertools import chain\n",
        "from typing import (List, Dict, Optional, Tuple, Union, Any,\n",
        "                   TypeVar, Callable, Iterable)\n",
        "from urllib.request import urlopen\n",
        "from io import BytesIO\n",
        "\n",
        "# Third-party general imports\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import plotly.express as px\n",
        "from tqdm import tqdm\n",
        "import threadpoolctl\n",
        "\n",
        "# Machine learning and data processing\n",
        "from sklearn.preprocessing import (StandardScaler, MultiLabelBinarizer,\n",
        "                                 normalize)\n",
        "from sklearn.cluster import (DBSCAN, AgglomerativeClustering)\n",
        "from sklearn.manifold import TSNE\n",
        "from scipy.spatial.distance import euclidean\n",
        "from scipy import sparse\n",
        "from scipy.sparse import hstack, csr_matrix\n",
        "\n",
        "# Deep learning and NLP\n",
        "import torch\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import timm\n",
        "\n",
        "# Image processing\n",
        "from PIL import Image\n",
        "import requests\n",
        "\n",
        "# Spark\n",
        "import pyspark\n",
        "from pyspark.sql import SparkSession, functions as F\n",
        "from pyspark.sql.functions import col, to_timestamp\n",
        "\n",
        "# Jupyter specific\n",
        "from IPython.display import display, Markdown\n",
        "\n",
        "# Suppress warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w6z47sb7Y1T-",
        "outputId": "c9fdce3f-5050-4423-e34d-a3468ee9159a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:lightning_fabric.utilities.seed:Seed set to 60\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "60"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "# Import libraries\n",
        "from lightning_fabric import seed_everything\n",
        "from pytorch_lightning import Trainer, LightningModule\n",
        "from pytorch_lightning.loggers import CSVLogger\n",
        "from pytorch_lightning.callbacks import EarlyStopping, ModelCheckpoint, Callback\n",
        "\n",
        "from rectools import Columns, ExternalIds\n",
        "from rectools.dataset import Dataset\n",
        "from rectools.metrics import NDCG, Recall, Serendipity, calc_metrics\n",
        "from rectools.models import BERT4RecModel, SASRecModel, load_model, PopularInCategoryModel\n",
        "from implicit.cpu.bpr import BayesianPersonalizedRanking\n",
        "from rectools.models.implicit_bpr import ImplicitBPRWrapperModel\n",
        "from rectools.models.nn.item_net import IdEmbeddingsItemNet\n",
        "from rectools.models.nn.transformers.base import TransformerModelBase\n",
        "\n",
        "from rectools import Columns\n",
        "from rectools.dataset import Dataset\n",
        "from rectools.metrics import (\n",
        "    MAP,\n",
        "    CoveredUsers,\n",
        "    AvgRecPopularity,\n",
        "    Intersection,\n",
        "    HitRate,\n",
        "    Serendipity,\n",
        ")\n",
        "from rectools.models import PopularModel, EASEModel, SASRecModel, BERT4RecModel\n",
        "from rectools.model_selection import TimeRangeSplitter, cross_validate\n",
        "from rectools.models.nn.item_net import CatFeaturesItemNet, IdEmbeddingsItemNet\n",
        "from rectools.visuals import MetricsApp\n",
        "\n",
        "warnings.simplefilter(\"ignore\")\n",
        "\n",
        "# Enable deterministic behaviour with CUDA >= 10.2\n",
        "os.environ[\"CUBLAS_WORKSPACE_CONFIG\"] = \":4096:8\"\n",
        "\n",
        "# Random seed\n",
        "RANDOM_STATE=60\n",
        "torch.use_deterministic_algorithms(True)\n",
        "seed_everything(RANDOM_STATE, workers=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iFCdVpTikuGw",
        "outputId": "bfdd741d-df74-4232-98e5-663fea753128"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Colab Notebooks/diploma/scripts\n"
          ]
        }
      ],
      "source": [
        "%cd \"/content/drive/MyDrive/Colab Notebooks/diploma/scripts/\"\n",
        "import process_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QqQvRVZqY4VS"
      },
      "outputs": [],
      "source": [
        "# Создаём SparkSession\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"PetCo_1\") \\\n",
        "    .getOrCreate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V4BGeehhYtwi"
      },
      "outputs": [],
      "source": [
        "HEAD_DIRECTORY = '/content/drive/MyDrive/Colab Notebooks/diploma/'"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Фукнции"
      ],
      "metadata": {
        "id": "lGTSa7aoZGsr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wsMoDnaaBtFg"
      },
      "outputs": [],
      "source": [
        "def generate_item_text_descriptions(\n",
        "    catalog: pd.DataFrame,\n",
        "    text_features: List[str]\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"Конкатенирует выбранные текстовые признаки товаров через пробел.\n",
        "\n",
        "    Args:\n",
        "        catalog: Исходный DataFrame с товарами.\n",
        "        text_features: Колонки для объединения (в порядке конкатенации).\n",
        "\n",
        "    Returns:\n",
        "        DataFrame с одной колонкой 'Text description'.\n",
        "    \"\"\"\n",
        "    text_descriptions = catalog[[text_features[0]]].copy()\n",
        "    text_descriptions.columns = ['Text description']\n",
        "\n",
        "    for feature in text_features[1:]:\n",
        "        text_descriptions['Text description'] += ' ' + catalog[feature].astype(str)\n",
        "\n",
        "    return text_descriptions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rEyp4kEuB5GR"
      },
      "outputs": [],
      "source": [
        "def generate_item_text_embeddings(catalog: pd.DataFrame, text_features: List[str], model: SentenceTransformer, file_to_save: str, save_frequency: int = 1000) -> np.ndarray:\n",
        "    \"\"\"Генерирует текстовые эмбеддинги товаров усреднением векторов описаний.\n",
        "\n",
        "    Args:\n",
        "        catalog: DataFrame с товарами и их текстовыми признаками\n",
        "        text_features: Список колонок для формирования описаний\n",
        "        model: Модель для генерации эмбеддингов\n",
        "        file_to_save: Путь для промежуточного сохранения результатов\n",
        "        save_frequency: Частота автосохранения (в количестве товаров)\n",
        "\n",
        "    Returns:\n",
        "        Массив эмбеддингов shape=(n_items, embedding_dim)\n",
        "    \"\"\"\n",
        "    item_descriptions = generate_item_text_descriptions(catalog, text_features)\n",
        "    embeddings = []\n",
        "\n",
        "    for i in tqdm(range(len(item_descriptions))):\n",
        "        desc = item_descriptions.iloc[i, 0]\n",
        "        embeddings.append(model.encode(desc))\n",
        "\n",
        "        if (i + 1) % save_frequency == 0:\n",
        "            with open(file_to_save, \"wb\") as f:\n",
        "                pickle.dump(np.array(embeddings), f)\n",
        "\n",
        "    return np.array(embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3a15LIZNCzVV"
      },
      "outputs": [],
      "source": [
        "def sparse_onehot_from_lists(\n",
        "    df: pd.DataFrame,\n",
        "    categorical_columns: List[str]\n",
        ") -> csr_matrix:\n",
        "    \"\"\"Преобразует категориальные столбцы со списками значений в sparse one-hot матрицу.\n",
        "\n",
        "    Обрабатывает:\n",
        "    - None/NaN → преобразует в пустые списки\n",
        "    - Списки значений → one-hot кодирование\n",
        "\n",
        "    Args:\n",
        "        df: DataFrame с исходными данными\n",
        "        categorical_columns: Столбцы для обработки (содержат списки значений)\n",
        "\n",
        "    Returns:\n",
        "        Объединённая sparse one-hot матрица в CSR формате\n",
        "    \"\"\"\n",
        "    sparse_matrices = []\n",
        "\n",
        "    for col in categorical_columns:\n",
        "        # Обработка пропусков и несписочных значений\n",
        "        cleaned = df[col].apply(lambda x: x if isinstance(x, list) else [])\n",
        "\n",
        "        # Sparse one-hot кодирование\n",
        "        mlb = MultiLabelBinarizer(sparse_output=True)\n",
        "        sparse_matrices.append(mlb.fit_transform(cleaned))\n",
        "\n",
        "    return hstack(sparse_matrices, format='csr')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0wLHFPXPEAaZ"
      },
      "outputs": [],
      "source": [
        "def generate_item_image_embeddings(\n",
        "    catalog: pd.DataFrame,\n",
        "    feature: str,\n",
        "    image_model: timm.models.mobilenetv3.MobileNetV3,\n",
        "    file_to_save: str\n",
        ") -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Генерирует эмбеддинги изображений товаров.\n",
        "\n",
        "    :param catalog: Датасет с товарами. Должен содержать столбец с URL изображений.\n",
        "    :param feature: Название столбца с URL изображений.\n",
        "    :param image_model: Предобученная модель для генерации эмбеддингов.\n",
        "    :param file_to_save: Путь для сохранения промежуточных результатов.\n",
        "    :return: Массив эмбеддингов изображений shape=(n_items, embedding_dim)\n",
        "    \"\"\"\n",
        "    item_image_embeddings = []\n",
        "    working_urls = 0\n",
        "\n",
        "    for i in tqdm(range(len(catalog))):\n",
        "        image_url = catalog[feature].iloc[i]\n",
        "\n",
        "        if check_image_url(image_url):\n",
        "            response = requests.get(image_url, stream=True, timeout=5)\n",
        "            response.raise_for_status()\n",
        "            img = Image.open(BytesIO(response.content))\n",
        "            if img.mode != 'RGB':\n",
        "                img = img.convert('RGB')\n",
        "\n",
        "            output = image_model.forward_features(transforms(img).unsqueeze(0))\n",
        "            embedding = image_model.forward_head(output, pre_logits=True).detach().numpy()[0, :]\n",
        "            working_urls += 1\n",
        "        else:\n",
        "            embedding = np.zeros(1024)\n",
        "\n",
        "        item_image_embeddings.append(embedding)\n",
        "\n",
        "        if i % 1000 == 0:\n",
        "            with open(file_to_save, \"wb\") as f:\n",
        "                pickle.dump(np.array(item_image_embeddings), f)\n",
        "\n",
        "    return np.array(item_image_embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SHffhdL2Eg8Z"
      },
      "outputs": [],
      "source": [
        "def create_user_histories(interactions_df: pd.DataFrame) -> Dict[int, List[int]]:\n",
        "    \"\"\"\n",
        "    Создает словарь историй пользователей на основе взаимодействий с товарами.\n",
        "\n",
        "    :param interactions_df: Датасет с взаимодействиями пользователей с товарами.\n",
        "                            Должен содержать столбцы: 'user_id', 'item_id'.\n",
        "    :return: Словарь {user_id: список товаров, с которыми взаимодействовал пользователь}\n",
        "    \"\"\"\n",
        "    user_histories = {}\n",
        "\n",
        "    # Группируем данные по user_id и собираем список item_id для каждого пользователя\n",
        "    for user_id, group in tqdm(interactions_df.groupby('user_id')):\n",
        "        user_histories[user_id] = group['item_id'].tolist()\n",
        "\n",
        "    return user_histories"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fqoaFnaZEzvI"
      },
      "outputs": [],
      "source": [
        "def aggregate_user_cat_embeddings(\n",
        "    users_histories: Dict[int, List[int]],\n",
        "    sparse_cat_features: sparse.csr_matrix,\n",
        "    itemid2index: Dict[int, int]\n",
        ") -> Tuple[np.ndarray, Dict[int, int]]:\n",
        "    \"\"\"\n",
        "    Агрегирует категориальные признаки товаров в эмбеддинги пользователей.\n",
        "\n",
        "    :param users_histories: Словарь {user_id: список item_id} с историями просмотров\n",
        "    :param sparse_cat_features: Sparse-матрица one-hot признаков товаров\n",
        "    :param itemid2index: Соответствие item_id индексам в sparse_cat_features\n",
        "    :return: Кортеж (матрица эмбеддингов пользователей, словарь user_id -> индекс)\n",
        "    \"\"\"\n",
        "    num_users = len(users_histories)\n",
        "    num_features = sparse_cat_features.shape[1]\n",
        "    user_embeddings = sparse.lil_matrix((num_users, num_features), dtype=np.float32)\n",
        "    userid2index = {}\n",
        "\n",
        "    i = 0\n",
        "    for user_id, item_ids in tqdm(users_histories.items()):\n",
        "        userid2index[user_id] = i\n",
        "        indeces = [itemid2index[item_id] for item_id in item_ids if item_id in itemid2index.keys()]\n",
        "        user_item_features = sparse_cat_features[indeces, :]\n",
        "\n",
        "        sum_features = user_item_features.sum(axis=0)\n",
        "        user_embeddings[i] = sum_features\n",
        "        i += 1\n",
        "\n",
        "    return user_embeddings.toarray(), userid2index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VJMm79ZUFGu8"
      },
      "outputs": [],
      "source": [
        "def aggregate_user_noncat_embeddings(\n",
        "    user_histories: Dict[int, List[int]],\n",
        "    item_embeddings: np.ndarray,\n",
        "    itemid2index: Dict[int, int]\n",
        ") -> Tuple[np.ndarray, int]:\n",
        "    \"\"\"\n",
        "    Генерирует эмбеддинги пользователей усреднением эмбеддингов товаров из их истории.\n",
        "\n",
        "    :param user_histories: {user_id: список item_id} - истории взаимодействий\n",
        "    :param item_embeddings: Массив эмбеддингов товаров shape=(n_items, embedding_dim)\n",
        "    :param itemid2index: {item_id: индекс} в item_embeddings\n",
        "    :return: Кортеж (массив эмбеддингов пользователей, количество пропущенных товаров)\n",
        "    \"\"\"\n",
        "    user_embeddings = []\n",
        "    missed_items = 0\n",
        "\n",
        "    for user_id, item_ids in tqdm(user_histories.items()):\n",
        "        item_vectors = []\n",
        "\n",
        "        for item_id in item_ids:\n",
        "            if item_id in itemid2index:\n",
        "                item_vectors.append(item_embeddings[itemid2index[item_id]])\n",
        "            else:\n",
        "                missed_items += 1\n",
        "\n",
        "        if item_vectors:\n",
        "            user_embeddings.append(np.mean(item_vectors, axis=0))\n",
        "        else:\n",
        "            user_embeddings.append(np.zeros(item_embeddings.shape[1]))\n",
        "\n",
        "    return np.array(user_embeddings), missed_items"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TeDGCUmDFb55"
      },
      "outputs": [],
      "source": [
        "def generate_user_embeddings(\n",
        "    actions: pd.DataFrame,\n",
        "    catalog: pd.DataFrame,\n",
        "    text_features: List[str],\n",
        "    cat_features: List[str],\n",
        "    image_column: str,\n",
        "    text_model: object,\n",
        "    image_model: object\n",
        ") -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Генерирует объединенные эмбеддинги пользователей на основе их взаимодействий с товарами.\n",
        "    Комбинирует текстовые, категориальные и визуальные эмбеддинги товаров.\n",
        "\n",
        "    :param actions: DataFrame с историей взаимодействий пользователей\n",
        "    :param catalog: DataFrame с каталогом товаров и их признаками\n",
        "    :param text_features: Список текстовых признаков для генерации эмбеддингов\n",
        "    :param cat_features: Список категориальных признаков для one-hot кодирования\n",
        "    :param image_column: Название столбца с URL изображений\n",
        "    :param text_model: Модель для генерации текстовых эмбеддингов\n",
        "    :param image_model: Модель для генерации визуальных эмбеддингов\n",
        "    :return: np.ndarray (объединенные эмбеддинги пользователей)\n",
        "    \"\"\"\n",
        "    # Загрузка предварительно сохраненных эмбеддингов\n",
        "    item_text_embeddings = pd.read_pickle(HEAD_DIRECTORY+'embeddings/items_names.pkl')\n",
        "    sparse_item_cat_embeddings = sparse_onehot_from_lists(df=catalog, categorical_columns=cat_features)\n",
        "    item_image_embeddings = pd.read_pickle(HEAD_DIRECTORY+'embeddings/items_images.pkl')\n",
        "\n",
        "    # Проверка согласованности данных\n",
        "    assert item_text_embeddings.shape[0] == sparse_item_cat_embeddings.shape[0] == item_image_embeddings.shape[0]\n",
        "\n",
        "    # Создание истории пользователей и mapping'ов\n",
        "    user_histories = create_user_histories(actions)\n",
        "    itemid2index = {catalog['item_id'].iloc[i]: i for i in range(len(catalog['item_id']))}\n",
        "\n",
        "    # Генерация отдельных эмбеддингов\n",
        "    user_text_embeddings, _ = aggregate_user_noncat_embeddings(user_histories, item_text_embeddings, itemid2index)\n",
        "    user_cat_embeddings, userid2index = aggregate_user_cat_embeddings(user_histories, sparse_item_cat_embeddings, itemid2index)\n",
        "    user_image_embeddings, _ = aggregate_user_noncat_embeddings(user_histories, item_image_embeddings, itemid2index)\n",
        "\n",
        "    # Объединение всех эмбеддингов\n",
        "    user_embeddings = np.hstack((user_text_embeddings, user_cat_embeddings, user_image_embeddings))\n",
        "\n",
        "    return user_embeddings, userid2index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MOsKv1Z4GrsJ"
      },
      "outputs": [],
      "source": [
        "def tsne(\n",
        "    vectors: Union[np.ndarray, List[List[float]]],\n",
        "    perplexity: int = 30,\n",
        "    random_state: int = 42\n",
        ") -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Применяет t-SNE для уменьшения размерности векторов до 2D.\n",
        "\n",
        "    Параметры:\n",
        "        vectors: Входные векторы (n_samples, n_features)\n",
        "        perplexity: Параметр perplexity для t-SNE (по умолчанию 30)\n",
        "        random_state: Seed для воспроизводимости (по умолчанию 42)\n",
        "\n",
        "    Возвращает:\n",
        "        Массив формы (n_samples, 2) с проекциями в 2D пространстве\n",
        "    \"\"\"\n",
        "    # Стандартизация данных\n",
        "    scaler = StandardScaler()\n",
        "    scaled_vectors = scaler.fit_transform(vectors)\n",
        "\n",
        "    # Применение t-SNE\n",
        "    tsne = TSNE(n_components=2, perplexity=perplexity, random_state=random_state)\n",
        "    return tsne.fit_transform(scaled_vectors)\n",
        "\n",
        "def visualize_tsne(\n",
        "    projected_vectors: np.ndarray,\n",
        "    title: str = 't-SNE visualization',\n",
        "    figsize: tuple = (10, 8),\n",
        "    point_size: int = 50\n",
        ") -> None:\n",
        "    \"\"\"\n",
        "    Визуализирует 2D проекции векторов.\n",
        "\n",
        "    Параметры:\n",
        "        projected_vectors: Массив проекций формы (n_samples, 2)\n",
        "        title: Заголовок графика (по умолчанию 't-SNE visualization')\n",
        "        figsize: Размер фигуры (по умолчанию (10, 8))\n",
        "        point_size: Размер точек на графике (по умолчанию 50)\n",
        "    \"\"\"\n",
        "    plt.figure(figsize=figsize)\n",
        "    sns.scatterplot(\n",
        "        x=projected_vectors[:, 0],\n",
        "        y=projected_vectors[:, 1],\n",
        "        s=point_size\n",
        "    )\n",
        "\n",
        "    plt.title(title)\n",
        "    plt.xlabel('t-SNE dimension 1')\n",
        "    plt.ylabel('t-SNE dimension 2')\n",
        "    plt.grid(True)\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def dbscan_clustering(data, eps=0.5, min_samples=5, tsne_for_high_dim=True):\n",
        "    \"\"\"\n",
        "    Выполняет кластеризацию DBSCAN и визуализирует результаты.\n",
        "\n",
        "    Параметры:\n",
        "    - data: numpy array (n_samples, n_features)\n",
        "    - eps: максимальное расстояние между соседями для DBSCAN\n",
        "    - min_samples: минимальное количество соседей для core точки\n",
        "    - visualize: если True, строит график кластеров\n",
        "\n",
        "    Возвращает:\n",
        "    - labels: метки кластеров (-1 - выбросы)\n",
        "    \"\"\"\n",
        "    # Стандартизация данных\n",
        "    scaler = StandardScaler()\n",
        "    scaled_data = scaler.fit_transform(data)\n",
        "\n",
        "    # Кластеризация DBSCAN\n",
        "    dbscan = DBSCAN(eps=eps, min_samples=min_samples)\n",
        "    labels = dbscan.fit_predict(scaled_data)\n",
        "\n",
        "    return scaled_data, labels\n",
        "\n",
        "\n",
        "def visualise_dbscan(scaled_data, labels, eps, min_samples):\n",
        "\n",
        "    userid2cluster = {}\n",
        "    for user_id, index in userid2index.items():\n",
        "      userid2cluster[user_id] = labels[index]\n",
        "\n",
        "    plt.figure(figsize=(10, 8))\n",
        "\n",
        "    vis_data = scaled_data[:, :2]\n",
        "    x_label, y_label = 't-SNE dimension 1', 't-SNE dimension 2'\n",
        "\n",
        "    # Визуализация\n",
        "    unique_labels = set(labels)\n",
        "    colors = plt.cm.Spectral(np.linspace(0, 1, len(unique_labels)))\n",
        "\n",
        "    for k, col in zip(unique_labels, colors):\n",
        "        if k == -1:\n",
        "            col = 'gray'  # Выбросы серым\n",
        "\n",
        "        class_member_mask = (labels == k)\n",
        "        xy = vis_data[class_member_mask]\n",
        "        plt.scatter(xy[:, 0], xy[:, 1], c=[col], s=50, label=f'Cluster {k}' if k != -1 else 'Outliers')\n",
        "\n",
        "    plt.title(f'DBSCAN Clustering (eps={eps}, min_samples={min_samples})')\n",
        "    plt.xlabel(x_label)\n",
        "    plt.ylabel(y_label)\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "    return userid2cluster"
      ],
      "metadata": {
        "id": "QEV89n6JY_6p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GOVYaJ2yKsey"
      },
      "outputs": [],
      "source": [
        "def show_image_from_url(url):\n",
        "    \"\"\"\n",
        "    Проверяет существование изображения по URL и выводит его на экран.\n",
        "\n",
        "    Параметры:\n",
        "        url (str): Ссылка на изображение в интернете.\n",
        "\n",
        "    Возвращает:\n",
        "        bool: True если изображение существует и отображено, иначе False.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Загружаем изображение с проверкой HTTP-статуса\n",
        "        response = requests.get(url, stream=True, timeout=5)\n",
        "        response.raise_for_status()  # Проверяет на ошибки HTTP (404, 403 и т.д.)\n",
        "\n",
        "        # Проверяем, что это действительно изображение\n",
        "        if 'image' not in response.headers['Content-Type']:\n",
        "            print(\"⚠️ URL не ведет к изображению (Content-Type:\", response.headers['Content-Type'], \")\")\n",
        "            return False\n",
        "\n",
        "        # Преобразуем байты в изображение и выводим\n",
        "        img = Image.open(BytesIO(response.content))\n",
        "\n",
        "        plt.figure(figsize=(8, 6))\n",
        "        plt.imshow(img)\n",
        "        plt.axis('off')  # Скрываем оси\n",
        "        plt.title(f\"Изображение из URL:\\n{url[:50]}...\", fontsize=10)\n",
        "        plt.show()\n",
        "        return img\n",
        "\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"❌ Ошибка при загрузке изображения: {e}\")\n",
        "        return False\n",
        "    except Exception as e:\n",
        "        print(f\"⚠️ Неожиданная ошибка: {e}\")\n",
        "        return False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T2d386fSK0Ab"
      },
      "outputs": [],
      "source": [
        "def check_image_url(url):\n",
        "    \"\"\"Проверяет, доступно ли изображение по URL.\"\"\"\n",
        "    try:\n",
        "        response = requests.head(url, timeout=5, allow_redirects=True)\n",
        "        if response.status_code == 200 and 'image' in response.headers.get('Content-Type', ''):\n",
        "            return True\n",
        "        return False\n",
        "    except:\n",
        "        return False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "34ELS4akLP_K"
      },
      "outputs": [],
      "source": [
        "def get_user_histories_by_cluster(\n",
        "    cluster: int,\n",
        "    clusters: Dict[int, int],\n",
        "    interactions_with_features: pd.DataFrame,\n",
        "    n_examples: int = 3\n",
        ") -> None:\n",
        "    \"\"\"\n",
        "    Выводит примеры пользовательских историй для заданного кластера.\n",
        "\n",
        "    Параметры:\n",
        "        cluster: Номер кластера для отображения примеров\n",
        "        clusters: Словарь {user_id: cluster_label} с принадлежностью к кластерам\n",
        "        interactions_with_features: DataFrame с историей взаимодействий пользователей\n",
        "        n_examples: Количество примеров для отображения (по умолчанию 3)\n",
        "\n",
        "    Возвращает:\n",
        "        None (функция только выводит информацию на экран)\n",
        "    \"\"\"\n",
        "    n_printed_examples = 0\n",
        "\n",
        "    for user_id, user_cluster in clusters.items():\n",
        "        if user_cluster == cluster:\n",
        "            display_user_actions(\n",
        "                user_id=user_id,\n",
        "                df=interactions_with_features,\n",
        "                n_cols=10,\n",
        "                show_text=True\n",
        "            )\n",
        "            n_printed_examples += 1\n",
        "\n",
        "            if n_printed_examples >= n_examples:\n",
        "                break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uMzazSqtL7dB"
      },
      "outputs": [],
      "source": [
        "def display_user_actions(\n",
        "    user_id: int,\n",
        "    df: pd.DataFrame,\n",
        "    n_cols: int = 5,\n",
        "    cell_width: float = 2.5,\n",
        "    cell_height: float = 3,\n",
        "    show_text: bool = True,\n",
        "    fontsize: int = 8,\n",
        "    max_name_length: int = 20,\n",
        "    timeout: float = 3.0,\n",
        "    image_url_column: str = 'image_url',\n",
        "    item_id_column: str = 'item_id',\n",
        "    name_column: str = 'name'\n",
        ") -> None:\n",
        "    \"\"\"\n",
        "    Визуализирует действия пользователя с фиксированным размером ячеек.\n",
        "\n",
        "    Параметры:\n",
        "        user_id: ID пользователя для отображения\n",
        "        df: DataFrame с данными о взаимодействиях\n",
        "        n_cols: Количество колонок в сетке (по умолчанию 5)\n",
        "        cell_width: Ширина ячейки в дюймах (по умолчанию 2.5)\n",
        "        cell_height: Высота ячейки в дюймах (по умолчанию 3)\n",
        "        show_text: Показывать текстовую информацию (по умолчанию True)\n",
        "        fontsize: Размер шрифта (по умолчанию 8)\n",
        "        max_name_length: Максимальная длина строки названия (по умолчанию 20)\n",
        "        timeout: Таймаут загрузки изображения в секундах (по умолчанию 3.0)\n",
        "        image_url_column: Название колонки с URL изображений (по умолчанию 'image_url')\n",
        "        item_id_column: Название колонки с ID товаров (по умолчанию 'item_id')\n",
        "        name_column: Название колонки с именами товаров (по умолчанию 'name')\n",
        "    \"\"\"\n",
        "    user_items = df[df['user_id'] == user_id]\n",
        "    if user_items.empty:\n",
        "        print(f\"Пользователь {user_id} не найден.\")\n",
        "        return\n",
        "\n",
        "    n_items = len(user_items)\n",
        "    n_rows = int(np.ceil(n_items / n_cols))\n",
        "    figsize = (n_cols * cell_width, n_rows * cell_height)\n",
        "    fig = plt.figure(figsize=figsize)\n",
        "    grid = plt.GridSpec(n_rows, n_cols, wspace=0.1, hspace=0.2)\n",
        "\n",
        "    for idx, (_, row) in enumerate(user_items.iterrows()):\n",
        "        ax = fig.add_subplot(grid[idx // n_cols, idx % n_cols])\n",
        "\n",
        "        try:\n",
        "            # Загрузка и обработка изображения\n",
        "            response = requests.get(row[image_url_column], timeout=timeout)\n",
        "            response.raise_for_status()\n",
        "            img = Image.open(BytesIO(response.content))\n",
        "\n",
        "            # Масштабирование с сохранением пропорций\n",
        "            img_aspect = img.size[0] / img.size[1]\n",
        "            display_width = min(cell_width * 0.9, cell_height * 0.8 * img_aspect)\n",
        "            display_height = display_width / img_aspect\n",
        "\n",
        "            ax.imshow(img, extent=[\n",
        "                (cell_width - display_width)/2,\n",
        "                (cell_width + display_width)/2,\n",
        "                (cell_height - display_height)/2,\n",
        "                (cell_height + display_height)/2\n",
        "            ])\n",
        "\n",
        "            if show_text:\n",
        "                name = '\\n'.join(textwrap.wrap(row[name_column], max_name_length))\n",
        "                ax.text(\n",
        "                    cell_width/2, -0.2*cell_height,\n",
        "                    f\"ID: {row[item_id_column]}\\n{name}\",\n",
        "                    ha='center', va='top',\n",
        "                    fontsize=fontsize, wrap=True\n",
        "                )\n",
        "\n",
        "        except Exception as e:\n",
        "            # Обработка ошибок загрузки изображения\n",
        "            ax.add_patch(plt.Rectangle(\n",
        "                (0, 0), cell_width, cell_height,\n",
        "                color='lightgray'\n",
        "            ))\n",
        "            if show_text:\n",
        "                ax.text(\n",
        "                    cell_width/2, cell_height/2, 'X',\n",
        "                    color='red', ha='center', va='center'\n",
        "                )\n",
        "                ax.text(\n",
        "                    cell_width/2, -0.2*cell_height,\n",
        "                    f\"ID: {row[item_id_column]}\",\n",
        "                    ha='center', va='top', fontsize=fontsize\n",
        "                )\n",
        "\n",
        "        ax.set_xlim(0, cell_width)\n",
        "        ax.set_ylim(-0.3*cell_height if show_text else 0, cell_height)\n",
        "        ax.axis('off')\n",
        "\n",
        "    # Скрытие пустых ячеек\n",
        "    for idx in range(n_items, n_rows * n_cols):\n",
        "        fig.add_subplot(grid[idx // n_cols, idx % n_cols]).axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Umd6xzxeN7HY"
      },
      "outputs": [],
      "source": [
        "class PopularInUserModel():\n",
        "    def __init__(self, k: int = 10):\n",
        "        \"\"\"\n",
        "        Рекомендательная система на основе популярности.\n",
        "        :param top_n: Количество рекомендаций на пользователя\n",
        "        \"\"\"\n",
        "        self.top_n = k\n",
        "        self.item_popularity = None\n",
        "        self.user_top_items = None\n",
        "\n",
        "    def fit(self, interactions: pd.DataFrame):\n",
        "        \"\"\"\n",
        "        Обучает модель на данных о взаимодействиях пользователей с товарами.\n",
        "        :param interactions: DataFrame с колонками ['user_id', 'item_id']\n",
        "        \"\"\"\n",
        "        # Подсчет популярности товаров\n",
        "        item_counts = interactions['item_id'].value_counts().reset_index()\n",
        "        item_counts.columns = ['item_id', 'score']\n",
        "        self.item_popularity = item_counts\n",
        "\n",
        "        # Топ-10 популярных товаров для каждого пользователя\n",
        "        user_top_items = (\n",
        "            interactions.groupby('user_id')['item_id']\n",
        "            .value_counts()\n",
        "            .reset_index(name='score')\n",
        "        )\n",
        "\n",
        "        self.user_top_items = user_top_items[user_top_items.groupby('user_id').cumcount() < self.top_n]\n",
        "\n",
        "    def recommend(self, user_ids: List[int], train_dataset = None, k = None, filter_viewed = None) -> pd.DataFrame:\n",
        "        \"\"\"\n",
        "        Возвращает DataFrame с рекомендациями в формате user_id, item_id, score, rank.\n",
        "        :param user_ids: Список ID пользователей\n",
        "        :return: DataFrame с рекомендациями\n",
        "        \"\"\"\n",
        "        if self.user_top_items is None or self.item_popularity is None:\n",
        "            raise ValueError(\"Модель не обучена. Вызовите .fit() перед рекомендациями.\")\n",
        "\n",
        "        recommendations = []\n",
        "        for user_id in user_ids:\n",
        "            user_items = self.user_top_items[self.user_top_items['user_id'] == user_id].sort_values(by='score', ascending=False)\n",
        "\n",
        "            # Дополняем недостающие рекомендации самыми популярными товарами\n",
        "            if len(user_items) < self.top_n:\n",
        "                additional_items = self.item_popularity[~self.item_popularity['item_id'].isin(user_items['item_id'])]\n",
        "                additional_items = additional_items.head(self.top_n - len(user_items)).copy().sort_values(by='score', ascending=False)\n",
        "                additional_items['user_id'] = user_id\n",
        "                user_items = pd.concat([user_items, additional_items], ignore_index=True)\n",
        "\n",
        "            user_items = user_items.head(self.top_n)\n",
        "            user_items['rank'] = range(1, len(user_items) + 1)\n",
        "            recommendations.append(user_items)\n",
        "\n",
        "        return pd.concat(recommendations, ignore_index=True)[['user_id', 'item_id', 'score', 'rank']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yeAgRbDjOC6E"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from collections import Counter\n",
        "\n",
        "def keep_top_10_clusters(labels):\n",
        "    \"\"\"\n",
        "    Оставляет только 10 самых многочисленных кластеров, остальные помечает как шум (-1).\n",
        "\n",
        "    Параметры:\n",
        "    - labels: массив с метками кластеров (например, результат DBSCAN)\n",
        "\n",
        "    Возвращает:\n",
        "    - Модифицированный массив меток, где только 10 самых больших кластеров сохранили свои номера,\n",
        "      а остальным присвоено -1\n",
        "    \"\"\"\n",
        "    # Считаем количество точек в каждом кластере (исключая шум -1)\n",
        "    cluster_counts = Counter(labels)\n",
        "    if -1 in cluster_counts:\n",
        "        del cluster_counts[-1]  # Игнорируем точки шума\n",
        "\n",
        "    # Если кластеров 10 или меньше - ничего не меняем\n",
        "    if len(cluster_counts) <= 10:\n",
        "        return labels.copy()\n",
        "\n",
        "    # Находим 10 самых больших кластеров\n",
        "    top_10_clusters = [cluster for cluster, _ in cluster_counts.most_common(10)]\n",
        "\n",
        "    # Создаем новый массив меток\n",
        "    new_labels = np.full_like(labels, -1)  # По умолчанию - шум\n",
        "\n",
        "    # Сохраняем только топ-10 кластеров\n",
        "    for cluster in top_10_clusters:\n",
        "        new_labels[labels == cluster] = cluster\n",
        "\n",
        "    return new_labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hPLStMa2OJBy"
      },
      "outputs": [],
      "source": [
        "def get_cluster_examples(userid2clusters: Dict[int, int]):\n",
        "  cluster_examples = {}\n",
        "  for cluster in np.unique(list(userid2clusters.values()))[1:]:\n",
        "    for user_id, user_cluster in userid2clusters.items():\n",
        "      if user_cluster == cluster:\n",
        "        cluster_examples[cluster] = user_id\n",
        "        break\n",
        "\n",
        "  return cluster_examples"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sLbB0JK9Im31"
      },
      "source": [
        "# Данные\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ux9A7EW0JDgQ"
      },
      "outputs": [],
      "source": [
        "# sasrec_data_folder_path = HEAD_DIRECTORY+'data/sasrec_format/'\n",
        "# output_name = 'actions'\n",
        "# data_actions_processed = spark.read.parquet(sasrec_data_folder_path+output_name).orderBy(['user_id', 'datetime'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NkSsC4U2I7qD"
      },
      "outputs": [],
      "source": [
        "# sasrec_data_folder_path = HEAD_DIRECTORY+'data/sasrec_format/'\n",
        "# output_name = 'items'\n",
        "# data_items_processed = spark.read.parquet(sasrec_data_folder_path+output_name).orderBy('id', 'feature', 'value')\n",
        "# data_items_cleaned = spark.read.parquet(HEAD_DIRECTORY+'data/cleaned_data/data_items')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oPuu7eorIxt6"
      },
      "outputs": [],
      "source": [
        "# actions, items_features = process_data.processed_datasets_to_pandas(data_actions_processed, data_items_processed)\n",
        "# actions.to_pickle(HEAD_DIRECTORY+'data/sasrec_format/actions.pkl')\n",
        "# items_features.to_pickle(HEAD_DIRECTORY+'data/sasrec_format/items.pkl')\n",
        "actions = pd.read_pickle(HEAD_DIRECTORY+'data/sasrec_format/actions.pkl')\n",
        "items = pd.read_pickle(HEAD_DIRECTORY+'data/sasrec_format/items.pkl')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}