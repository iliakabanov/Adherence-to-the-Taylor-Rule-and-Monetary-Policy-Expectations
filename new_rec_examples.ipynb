{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPr7qQrTtU7ltF6SQcQf2Gc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/iliakabanov/Adherence-to-the-Taylor-Rule-and-Monetary-Policy-Expectations/blob/main/new_rec_examples.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Устанавливаем окружение"
      ],
      "metadata": {
        "id": "zq3Ws6j5jd5k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip cache purge\n",
        "!pip uninstall -y numpy scipy rectools\n",
        "!pip install \"numpy<1.29.0\" \"scipy<1.11.0\" rectools\n",
        "!pip install lightning_fabric\n",
        "!pip install rectools\n",
        "!pip install rectools[torch]\n",
        "!pip install gensim\n",
        "!pip install -U sentence-transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vf8LPAa4jdpe",
        "outputId": "3df163b8-ed69-42eb-d852-01eb7a094fe0"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mWARNING: No matching packages\u001b[0m\u001b[33m\n",
            "\u001b[0mFiles removed: 0\n",
            "Found existing installation: numpy 2.0.2\n",
            "Uninstalling numpy-2.0.2:\n",
            "  Successfully uninstalled numpy-2.0.2\n",
            "Found existing installation: scipy 1.14.1\n",
            "Uninstalling scipy-1.14.1:\n",
            "  Successfully uninstalled scipy-1.14.1\n",
            "\u001b[33mWARNING: Skipping rectools as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0mCollecting numpy<1.29.0\n",
            "  Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting scipy<1.11.0\n",
            "  Downloading scipy-1.10.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.9/58.9 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting rectools\n",
            "  Downloading rectools-0.13.0-py3-none-any.whl.metadata (16 kB)\n",
            "Collecting attrs<24.0.0,>=19.1.0 (from rectools)\n",
            "  Downloading attrs-23.2.0-py3-none-any.whl.metadata (9.5 kB)\n",
            "Collecting implicit<0.8.0,>=0.7.1 (from rectools)\n",
            "  Downloading implicit-0.7.2-cp311-cp311-manylinux2014_x86_64.whl.metadata (6.1 kB)\n",
            "Requirement already satisfied: pandas<3.0.0,>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from rectools) (2.2.2)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from rectools) (2.11.2)\n",
            "Requirement already satisfied: pydantic-core<3.0.0,>=2.20.1 in /usr/local/lib/python3.11/dist-packages (from rectools) (2.33.1)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.27.0 in /usr/local/lib/python3.11/dist-packages (from rectools) (4.67.1)\n",
            "Requirement already satisfied: typeguard<5.0.0,>=4.1.0 in /usr/local/lib/python3.11/dist-packages (from rectools) (4.4.2)\n",
            "Requirement already satisfied: typing-extensions<5.0.0,>=4.12.2 in /usr/local/lib/python3.11/dist-packages (from rectools) (4.13.1)\n",
            "Requirement already satisfied: threadpoolctl in /usr/local/lib/python3.11/dist-packages (from implicit<0.8.0,>=0.7.1->rectools) (3.6.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0.0,>=1.5.0->rectools) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0.0,>=1.5.0->rectools) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0.0,>=1.5.0->rectools) (2025.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.8.2->rectools) (0.7.0)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.8.2->rectools) (0.4.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas<3.0.0,>=1.5.0->rectools) (1.17.0)\n",
            "Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m44.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading scipy-1.10.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (34.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m34.1/34.1 MB\u001b[0m \u001b[31m17.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading rectools-0.13.0-py3-none-any.whl (207 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.0/207.0 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading attrs-23.2.0-py3-none-any.whl (60 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.8/60.8 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading implicit-0.7.2-cp311-cp311-manylinux2014_x86_64.whl (8.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.9/8.9 MB\u001b[0m \u001b[31m46.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: numpy, attrs, scipy, implicit, rectools\n",
            "  Attempting uninstall: attrs\n",
            "    Found existing installation: attrs 25.3.0\n",
            "    Uninstalling attrs-25.3.0:\n",
            "      Successfully uninstalled attrs-25.3.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "jaxlib 0.5.1 requires scipy>=1.11.1, but you have scipy 1.10.1 which is incompatible.\n",
            "scikit-image 0.25.2 requires scipy>=1.11.4, but you have scipy 1.10.1 which is incompatible.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\n",
            "cvxpy 1.6.4 requires scipy>=1.11.0, but you have scipy 1.10.1 which is incompatible.\n",
            "jax 0.5.2 requires scipy>=1.11.1, but you have scipy 1.10.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed attrs-23.2.0 implicit-0.7.2 numpy-1.26.4 rectools-0.13.0 scipy-1.10.1\n",
            "Collecting lightning_fabric\n",
            "  Downloading lightning_fabric-2.5.1-py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: torch>=2.1.0 in /usr/local/lib/python3.11/dist-packages (from lightning_fabric) (2.6.0+cu124)\n",
            "Requirement already satisfied: fsspec>=2022.5.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2022.5.0->lightning_fabric) (2025.3.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from lightning_fabric) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.4.0 in /usr/local/lib/python3.11/dist-packages (from lightning_fabric) (4.13.1)\n",
            "Collecting lightning-utilities>=0.10.0 (from lightning_fabric)\n",
            "  Downloading lightning_utilities-0.14.3-py3-none-any.whl.metadata (5.6 kB)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2022.5.0->lightning_fabric) (3.11.15)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from lightning-utilities>=0.10.0->lightning_fabric) (75.2.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->lightning_fabric) (3.18.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->lightning_fabric) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->lightning_fabric) (3.1.6)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=2.1.0->lightning_fabric)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=2.1.0->lightning_fabric)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=2.1.0->lightning_fabric)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=2.1.0->lightning_fabric)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=2.1.0->lightning_fabric)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=2.1.0->lightning_fabric)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=2.1.0->lightning_fabric)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=2.1.0->lightning_fabric)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=2.1.0->lightning_fabric)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->lightning_fabric) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->lightning_fabric) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->lightning_fabric) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=2.1.0->lightning_fabric)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->lightning_fabric) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->lightning_fabric) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.1.0->lightning_fabric) (1.3.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->lightning_fabric) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->lightning_fabric) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->lightning_fabric) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->lightning_fabric) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->lightning_fabric) (6.3.2)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->lightning_fabric) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->lightning_fabric) (1.18.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.1.0->lightning_fabric) (3.0.2)\n",
            "Requirement already satisfied: idna>=2.0 in /usr/local/lib/python3.11/dist-packages (from yarl<2.0,>=1.17.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->lightning_fabric) (3.10)\n",
            "Downloading lightning_fabric-2.5.1-py3-none-any.whl (250 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m250.7/250.7 kB\u001b[0m \u001b[31m16.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lightning_utilities-0.14.3-py3-none-any.whl (28 kB)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m88.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m69.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m44.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m70.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, lightning-utilities, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, lightning_fabric\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed lightning-utilities-0.14.3 lightning_fabric-2.5.1 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n",
            "Requirement already satisfied: rectools in /usr/local/lib/python3.11/dist-packages (0.13.0)\n",
            "Requirement already satisfied: attrs<24.0.0,>=19.1.0 in /usr/local/lib/python3.11/dist-packages (from rectools) (23.2.0)\n",
            "Requirement already satisfied: implicit<0.8.0,>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from rectools) (0.7.2)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.22 in /usr/local/lib/python3.11/dist-packages (from rectools) (1.26.4)\n",
            "Requirement already satisfied: pandas<3.0.0,>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from rectools) (2.2.2)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from rectools) (2.11.2)\n",
            "Requirement already satisfied: pydantic-core<3.0.0,>=2.20.1 in /usr/local/lib/python3.11/dist-packages (from rectools) (2.33.1)\n",
            "Requirement already satisfied: scipy<1.13,>=1.10.1 in /usr/local/lib/python3.11/dist-packages (from rectools) (1.10.1)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.27.0 in /usr/local/lib/python3.11/dist-packages (from rectools) (4.67.1)\n",
            "Requirement already satisfied: typeguard<5.0.0,>=4.1.0 in /usr/local/lib/python3.11/dist-packages (from rectools) (4.4.2)\n",
            "Requirement already satisfied: typing-extensions<5.0.0,>=4.12.2 in /usr/local/lib/python3.11/dist-packages (from rectools) (4.13.1)\n",
            "Requirement already satisfied: threadpoolctl in /usr/local/lib/python3.11/dist-packages (from implicit<0.8.0,>=0.7.1->rectools) (3.6.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0.0,>=1.5.0->rectools) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0.0,>=1.5.0->rectools) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0.0,>=1.5.0->rectools) (2025.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.8.2->rectools) (0.7.0)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.8.2->rectools) (0.4.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas<3.0.0,>=1.5.0->rectools) (1.17.0)\n",
            "Requirement already satisfied: rectools[torch] in /usr/local/lib/python3.11/dist-packages (0.13.0)\n",
            "Requirement already satisfied: attrs<24.0.0,>=19.1.0 in /usr/local/lib/python3.11/dist-packages (from rectools[torch]) (23.2.0)\n",
            "Requirement already satisfied: implicit<0.8.0,>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from rectools[torch]) (0.7.2)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.22 in /usr/local/lib/python3.11/dist-packages (from rectools[torch]) (1.26.4)\n",
            "Requirement already satisfied: pandas<3.0.0,>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from rectools[torch]) (2.2.2)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from rectools[torch]) (2.11.2)\n",
            "Requirement already satisfied: pydantic-core<3.0.0,>=2.20.1 in /usr/local/lib/python3.11/dist-packages (from rectools[torch]) (2.33.1)\n",
            "Collecting pytorch-lightning<3.0.0,>=1.6.0 (from rectools[torch])\n",
            "  Downloading pytorch_lightning-2.5.1-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: scipy<1.13,>=1.10.1 in /usr/local/lib/python3.11/dist-packages (from rectools[torch]) (1.10.1)\n",
            "Requirement already satisfied: torch<3.0.0,>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from rectools[torch]) (2.6.0+cu124)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.27.0 in /usr/local/lib/python3.11/dist-packages (from rectools[torch]) (4.67.1)\n",
            "Requirement already satisfied: typeguard<5.0.0,>=4.1.0 in /usr/local/lib/python3.11/dist-packages (from rectools[torch]) (4.4.2)\n",
            "Requirement already satisfied: typing-extensions<5.0.0,>=4.12.2 in /usr/local/lib/python3.11/dist-packages (from rectools[torch]) (4.13.1)\n",
            "Requirement already satisfied: threadpoolctl in /usr/local/lib/python3.11/dist-packages (from implicit<0.8.0,>=0.7.1->rectools[torch]) (3.6.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0.0,>=1.5.0->rectools[torch]) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0.0,>=1.5.0->rectools[torch]) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0.0,>=1.5.0->rectools[torch]) (2025.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.8.2->rectools[torch]) (0.7.0)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.8.2->rectools[torch]) (0.4.0)\n",
            "Requirement already satisfied: PyYAML>=5.4 in /usr/local/lib/python3.11/dist-packages (from pytorch-lightning<3.0.0,>=1.6.0->rectools[torch]) (6.0.2)\n",
            "Requirement already satisfied: fsspec>=2022.5.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2022.5.0->pytorch-lightning<3.0.0,>=1.6.0->rectools[torch]) (2025.3.2)\n",
            "Collecting torchmetrics>=0.7.0 (from pytorch-lightning<3.0.0,>=1.6.0->rectools[torch])\n",
            "  Downloading torchmetrics-1.7.1-py3-none-any.whl.metadata (21 kB)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from pytorch-lightning<3.0.0,>=1.6.0->rectools[torch]) (24.2)\n",
            "Requirement already satisfied: lightning-utilities>=0.10.0 in /usr/local/lib/python3.11/dist-packages (from pytorch-lightning<3.0.0,>=1.6.0->rectools[torch]) (0.14.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch<3.0.0,>=1.6.0->rectools[torch]) (3.18.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch<3.0.0,>=1.6.0->rectools[torch]) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch<3.0.0,>=1.6.0->rectools[torch]) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3.0.0,>=1.6.0->rectools[torch]) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3.0.0,>=1.6.0->rectools[torch]) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3.0.0,>=1.6.0->rectools[torch]) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch<3.0.0,>=1.6.0->rectools[torch]) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch<3.0.0,>=1.6.0->rectools[torch]) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch<3.0.0,>=1.6.0->rectools[torch]) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch<3.0.0,>=1.6.0->rectools[torch]) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch<3.0.0,>=1.6.0->rectools[torch]) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch<3.0.0,>=1.6.0->rectools[torch]) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch<3.0.0,>=1.6.0->rectools[torch]) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch<3.0.0,>=1.6.0->rectools[torch]) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3.0.0,>=1.6.0->rectools[torch]) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3.0.0,>=1.6.0->rectools[torch]) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch<3.0.0,>=1.6.0->rectools[torch]) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch<3.0.0,>=1.6.0->rectools[torch]) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch<3.0.0,>=1.6.0->rectools[torch]) (1.3.0)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2022.5.0->pytorch-lightning<3.0.0,>=1.6.0->rectools[torch]) (3.11.15)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from lightning-utilities>=0.10.0->pytorch-lightning<3.0.0,>=1.6.0->rectools[torch]) (75.2.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas<3.0.0,>=1.5.0->rectools[torch]) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch<3.0.0,>=1.6.0->rectools[torch]) (3.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning<3.0.0,>=1.6.0->rectools[torch]) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning<3.0.0,>=1.6.0->rectools[torch]) (1.3.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning<3.0.0,>=1.6.0->rectools[torch]) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning<3.0.0,>=1.6.0->rectools[torch]) (6.3.2)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning<3.0.0,>=1.6.0->rectools[torch]) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning<3.0.0,>=1.6.0->rectools[torch]) (1.18.3)\n",
            "Requirement already satisfied: idna>=2.0 in /usr/local/lib/python3.11/dist-packages (from yarl<2.0,>=1.17.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning<3.0.0,>=1.6.0->rectools[torch]) (3.10)\n",
            "Downloading pytorch_lightning-2.5.1-py3-none-any.whl (822 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.0/823.0 kB\u001b[0m \u001b[31m40.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torchmetrics-1.7.1-py3-none-any.whl (961 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m961.5/961.5 kB\u001b[0m \u001b[31m47.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: torchmetrics, pytorch-lightning\n",
            "Successfully installed pytorch-lightning-2.5.1 torchmetrics-1.7.1\n",
            "Collecting gensim\n",
            "  Downloading gensim-4.3.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.1 kB)\n",
            "Requirement already satisfied: numpy<2.0,>=1.18.5 in /usr/local/lib/python3.11/dist-packages (from gensim) (1.26.4)\n",
            "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from gensim) (1.10.1)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.11/dist-packages (from gensim) (7.1.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open>=1.8.1->gensim) (1.17.2)\n",
            "Downloading gensim-4.3.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.7/26.7 MB\u001b[0m \u001b[31m67.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: gensim\n",
            "Successfully installed gensim-4.3.3\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.11/dist-packages (3.4.1)\n",
            "Collecting sentence-transformers\n",
            "  Downloading sentence_transformers-4.0.2-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.50.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.67.1)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (2.6.0+cu124)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.10.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (0.30.1)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (11.1.0)\n",
            "Requirement already satisfied: typing_extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.13.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.3.2)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (1.26.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.5.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2025.1.31)\n",
            "Downloading sentence_transformers-4.0.2-py3-none-any.whl (340 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m340.6/340.6 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: sentence-transformers\n",
            "  Attempting uninstall: sentence-transformers\n",
            "    Found existing installation: sentence-transformers 3.4.1\n",
            "    Uninstalling sentence-transformers-3.4.1:\n",
            "      Successfully uninstalled sentence-transformers-3.4.1\n",
            "Successfully installed sentence-transformers-4.0.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Имортируем библиотеки и запускаем spark-сессию"
      ],
      "metadata": {
        "id": "gpOTzBRqjs8I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get access to Google disk\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "metadata": {
        "id": "k0-s62f3jdmJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bSRv58o8YFiY"
      },
      "outputs": [],
      "source": [
        "# Standard library imports\n",
        "import os\n",
        "import re\n",
        "import pickle\n",
        "import warnings\n",
        "import textwrap\n",
        "from pathlib import Path\n",
        "from collections import Counter\n",
        "from itertools import chain\n",
        "from typing import (List, Dict, Optional, Tuple, Union, Any,\n",
        "                   TypeVar, Callable, Iterable)\n",
        "from urllib.request import urlopen\n",
        "from io import BytesIO\n",
        "\n",
        "# Third-party general imports\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import plotly.express as px\n",
        "from tqdm import tqdm\n",
        "import threadpoolctl\n",
        "\n",
        "# Machine learning and data processing\n",
        "from sklearn.preprocessing import (StandardScaler, MultiLabelBinarizer,\n",
        "                                 normalize)\n",
        "from sklearn.cluster import (DBSCAN, AgglomerativeClustering)\n",
        "from sklearn.manifold import TSNE\n",
        "from scipy.spatial.distance import euclidean\n",
        "from scipy import sparse\n",
        "from scipy.sparse import hstack, csr_matrix\n",
        "\n",
        "# Deep learning and NLP\n",
        "import torch\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import timm\n",
        "\n",
        "# Image processing\n",
        "from PIL import Image\n",
        "import requests\n",
        "\n",
        "# Spark\n",
        "import pyspark\n",
        "from pyspark.sql import SparkSession, functions as F\n",
        "from pyspark.sql.functions import col, to_timestamp\n",
        "\n",
        "# Jupyter specific\n",
        "from IPython.display import display, Markdown\n",
        "\n",
        "# Suppress warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w6z47sb7Y1T-",
        "outputId": "5a3854ea-7248-4e38-eb11-8b31d75d8c7e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:lightning_fabric.utilities.seed:Seed set to 60\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "60"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "# Import libraries\n",
        "from lightning_fabric import seed_everything\n",
        "from pytorch_lightning import Trainer, LightningModule\n",
        "from pytorch_lightning.loggers import CSVLogger\n",
        "from pytorch_lightning.callbacks import EarlyStopping, ModelCheckpoint, Callback\n",
        "\n",
        "from rectools import Columns, ExternalIds\n",
        "from rectools.dataset import Dataset\n",
        "from rectools.metrics import NDCG, Recall, Serendipity, calc_metrics\n",
        "from rectools.models import BERT4RecModel, SASRecModel, load_model, PopularInCategoryModel\n",
        "from implicit.cpu.bpr import BayesianPersonalizedRanking\n",
        "from rectools.models.implicit_bpr import ImplicitBPRWrapperModel\n",
        "from implicit.cpu.als import AlternatingLeastSquares\n",
        "from rectools.models.implicit_als import ImplicitALSWrapperModel\n",
        "from rectools.models.nn.item_net import IdEmbeddingsItemNet\n",
        "from rectools.models.nn.transformers.base import TransformerModelBase\n",
        "\n",
        "from rectools import Columns\n",
        "from rectools.dataset import Dataset\n",
        "from rectools.metrics import (\n",
        "    MAP,\n",
        "    CoveredUsers,\n",
        "    AvgRecPopularity,\n",
        "    Intersection,\n",
        "    HitRate,\n",
        "    Serendipity,\n",
        ")\n",
        "from rectools.models import PopularModel, EASEModel, SASRecModel, BERT4RecModel\n",
        "from rectools.model_selection import TimeRangeSplitter, cross_validate\n",
        "from rectools.models.nn.item_net import CatFeaturesItemNet, IdEmbeddingsItemNet\n",
        "from rectools.visuals import MetricsApp\n",
        "\n",
        "warnings.simplefilter(\"ignore\")\n",
        "\n",
        "# Enable deterministic behaviour with CUDA >= 10.2\n",
        "os.environ[\"CUBLAS_WORKSPACE_CONFIG\"] = \":4096:8\"\n",
        "\n",
        "# Random seed\n",
        "RANDOM_STATE=60\n",
        "torch.use_deterministic_algorithms(True)\n",
        "seed_everything(RANDOM_STATE, workers=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iFCdVpTikuGw",
        "outputId": "4d5c9414-f6ce-44a9-d33c-bec95e451a60"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Colab Notebooks/diploma/scripts\n"
          ]
        }
      ],
      "source": [
        "%cd \"/content/drive/MyDrive/Colab Notebooks/diploma/scripts/\"\n",
        "import process_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QqQvRVZqY4VS"
      },
      "outputs": [],
      "source": [
        "# Создаём SparkSession\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"PetCo_1\") \\\n",
        "    .getOrCreate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V4BGeehhYtwi"
      },
      "outputs": [],
      "source": [
        "HEAD_DIRECTORY = '/content/drive/MyDrive/Colab Notebooks/diploma/'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lGTSa7aoZGsr"
      },
      "source": [
        "# Фукнции"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wsMoDnaaBtFg"
      },
      "outputs": [],
      "source": [
        "def generate_item_text_descriptions(\n",
        "    catalog: pd.DataFrame,\n",
        "    text_features: List[str]\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"Конкатенирует выбранные текстовые признаки товаров через пробел.\n",
        "\n",
        "    Args:\n",
        "        catalog: Исходный DataFrame с товарами.\n",
        "        text_features: Колонки для объединения (в порядке конкатенации).\n",
        "\n",
        "    Returns:\n",
        "        DataFrame с одной колонкой 'Text description'.\n",
        "    \"\"\"\n",
        "    text_descriptions = catalog[[text_features[0]]].copy()\n",
        "    text_descriptions.columns = ['Text description']\n",
        "\n",
        "    for feature in text_features[1:]:\n",
        "        text_descriptions['Text description'] += ' ' + catalog[feature].astype(str)\n",
        "\n",
        "    return text_descriptions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rEyp4kEuB5GR"
      },
      "outputs": [],
      "source": [
        "def generate_item_text_embeddings(catalog: pd.DataFrame, text_features: List[str], model: SentenceTransformer, file_to_save: str, save_frequency: int = 1000) -> np.ndarray:\n",
        "    \"\"\"Генерирует текстовые эмбеддинги товаров усреднением векторов описаний.\n",
        "\n",
        "    Args:\n",
        "        catalog: DataFrame с товарами и их текстовыми признаками\n",
        "        text_features: Список колонок для формирования описаний\n",
        "        model: Модель для генерации эмбеддингов\n",
        "        file_to_save: Путь для промежуточного сохранения результатов\n",
        "        save_frequency: Частота автосохранения (в количестве товаров)\n",
        "\n",
        "    Returns:\n",
        "        Массив эмбеддингов shape=(n_items, embedding_dim)\n",
        "    \"\"\"\n",
        "    item_descriptions = generate_item_text_descriptions(catalog, text_features)\n",
        "    embeddings = []\n",
        "\n",
        "    for i in tqdm(range(len(item_descriptions))):\n",
        "        desc = item_descriptions.iloc[i, 0]\n",
        "        embeddings.append(model.encode(desc))\n",
        "\n",
        "        if (i + 1) % save_frequency == 0:\n",
        "            with open(file_to_save, \"wb\") as f:\n",
        "                pickle.dump(np.array(embeddings), f)\n",
        "\n",
        "    return np.array(embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3a15LIZNCzVV"
      },
      "outputs": [],
      "source": [
        "def sparse_onehot_from_lists(\n",
        "    df: pd.DataFrame,\n",
        "    categorical_columns: List[str]\n",
        ") -> csr_matrix:\n",
        "    \"\"\"Преобразует категориальные столбцы со списками значений в sparse one-hot матрицу.\n",
        "\n",
        "    Обрабатывает:\n",
        "    - None/NaN → преобразует в пустые списки\n",
        "    - Списки значений → one-hot кодирование\n",
        "\n",
        "    Args:\n",
        "        df: DataFrame с исходными данными\n",
        "        categorical_columns: Столбцы для обработки (содержат списки значений)\n",
        "\n",
        "    Returns:\n",
        "        Объединённая sparse one-hot матрица в CSR формате\n",
        "    \"\"\"\n",
        "    sparse_matrices = []\n",
        "\n",
        "    for col in categorical_columns:\n",
        "        # Обработка пропусков и несписочных значений\n",
        "        cleaned = df[col].apply(lambda x: x if isinstance(x, list) else [])\n",
        "\n",
        "        # Sparse one-hot кодирование\n",
        "        mlb = MultiLabelBinarizer(sparse_output=True)\n",
        "        sparse_matrices.append(mlb.fit_transform(cleaned))\n",
        "\n",
        "    return hstack(sparse_matrices, format='csr')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0wLHFPXPEAaZ"
      },
      "outputs": [],
      "source": [
        "def generate_item_image_embeddings(\n",
        "    catalog: pd.DataFrame,\n",
        "    feature: str,\n",
        "    image_model: timm.models.mobilenetv3.MobileNetV3,\n",
        "    file_to_save: str\n",
        ") -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Генерирует эмбеддинги изображений товаров.\n",
        "\n",
        "    :param catalog: Датасет с товарами. Должен содержать столбец с URL изображений.\n",
        "    :param feature: Название столбца с URL изображений.\n",
        "    :param image_model: Предобученная модель для генерации эмбеддингов.\n",
        "    :param file_to_save: Путь для сохранения промежуточных результатов.\n",
        "    :return: Массив эмбеддингов изображений shape=(n_items, embedding_dim)\n",
        "    \"\"\"\n",
        "    item_image_embeddings = []\n",
        "    working_urls = 0\n",
        "\n",
        "    for i in tqdm(range(len(catalog))):\n",
        "        image_url = catalog[feature].iloc[i]\n",
        "\n",
        "        if check_image_url(image_url):\n",
        "            response = requests.get(image_url, stream=True, timeout=5)\n",
        "            response.raise_for_status()\n",
        "            img = Image.open(BytesIO(response.content))\n",
        "            if img.mode != 'RGB':\n",
        "                img = img.convert('RGB')\n",
        "\n",
        "            output = image_model.forward_features(transforms(img).unsqueeze(0))\n",
        "            embedding = image_model.forward_head(output, pre_logits=True).detach().numpy()[0, :]\n",
        "            working_urls += 1\n",
        "        else:\n",
        "            embedding = np.zeros(1024)\n",
        "\n",
        "        item_image_embeddings.append(embedding)\n",
        "\n",
        "        if i % 1000 == 0:\n",
        "            with open(file_to_save, \"wb\") as f:\n",
        "                pickle.dump(np.array(item_image_embeddings), f)\n",
        "\n",
        "    return np.array(item_image_embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SHffhdL2Eg8Z"
      },
      "outputs": [],
      "source": [
        "def create_user_histories(interactions_df: pd.DataFrame) -> Dict[int, List[int]]:\n",
        "    \"\"\"\n",
        "    Создает словарь историй пользователей на основе взаимодействий с товарами.\n",
        "\n",
        "    :param interactions_df: Датасет с взаимодействиями пользователей с товарами.\n",
        "                            Должен содержать столбцы: 'user_id', 'item_id'.\n",
        "    :return: Словарь {user_id: список товаров, с которыми взаимодействовал пользователь}\n",
        "    \"\"\"\n",
        "    user_histories = {}\n",
        "\n",
        "    # Группируем данные по user_id и собираем список item_id для каждого пользователя\n",
        "    for user_id, group in tqdm(interactions_df.groupby('user_id')):\n",
        "        user_histories[user_id] = group['item_id'].tolist()\n",
        "\n",
        "    return user_histories"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fqoaFnaZEzvI"
      },
      "outputs": [],
      "source": [
        "def aggregate_user_cat_embeddings(\n",
        "    users_histories: Dict[int, List[int]],\n",
        "    sparse_cat_features: sparse.csr_matrix,\n",
        "    itemid2index: Dict[int, int]\n",
        ") -> Tuple[np.ndarray, Dict[int, int]]:\n",
        "    \"\"\"\n",
        "    Агрегирует категориальные признаки товаров в эмбеддинги пользователей.\n",
        "\n",
        "    :param users_histories: Словарь {user_id: список item_id} с историями просмотров\n",
        "    :param sparse_cat_features: Sparse-матрица one-hot признаков товаров\n",
        "    :param itemid2index: Соответствие item_id индексам в sparse_cat_features\n",
        "    :return: Кортеж (матрица эмбеддингов пользователей, словарь user_id -> индекс)\n",
        "    \"\"\"\n",
        "    num_users = len(users_histories)\n",
        "    num_features = sparse_cat_features.shape[1]\n",
        "    user_embeddings = sparse.lil_matrix((num_users, num_features), dtype=np.float32)\n",
        "    userid2index = {}\n",
        "\n",
        "    i = 0\n",
        "    for user_id, item_ids in tqdm(users_histories.items()):\n",
        "        userid2index[user_id] = i\n",
        "        indeces = [itemid2index[item_id] for item_id in item_ids if item_id in itemid2index.keys()]\n",
        "        user_item_features = sparse_cat_features[indeces, :]\n",
        "\n",
        "        sum_features = user_item_features.sum(axis=0)\n",
        "        user_embeddings[i] = sum_features\n",
        "        i += 1\n",
        "\n",
        "    return user_embeddings.toarray(), userid2index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VJMm79ZUFGu8"
      },
      "outputs": [],
      "source": [
        "def aggregate_user_noncat_embeddings(\n",
        "    user_histories: Dict[int, List[int]],\n",
        "    item_embeddings: np.ndarray,\n",
        "    itemid2index: Dict[int, int]\n",
        ") -> Tuple[np.ndarray, int]:\n",
        "    \"\"\"\n",
        "    Генерирует эмбеддинги пользователей усреднением эмбеддингов товаров из их истории.\n",
        "\n",
        "    :param user_histories: {user_id: список item_id} - истории взаимодействий\n",
        "    :param item_embeddings: Массив эмбеддингов товаров shape=(n_items, embedding_dim)\n",
        "    :param itemid2index: {item_id: индекс} в item_embeddings\n",
        "    :return: Кортеж (массив эмбеддингов пользователей, количество пропущенных товаров)\n",
        "    \"\"\"\n",
        "    user_embeddings = []\n",
        "    missed_items = 0\n",
        "\n",
        "    for user_id, item_ids in tqdm(user_histories.items()):\n",
        "        item_vectors = []\n",
        "\n",
        "        for item_id in item_ids:\n",
        "            if item_id in itemid2index:\n",
        "                item_vectors.append(item_embeddings[itemid2index[item_id]])\n",
        "            else:\n",
        "                missed_items += 1\n",
        "\n",
        "        if item_vectors:\n",
        "            user_embeddings.append(np.mean(item_vectors, axis=0))\n",
        "        else:\n",
        "            user_embeddings.append(np.zeros(item_embeddings.shape[1]))\n",
        "\n",
        "    return np.array(user_embeddings), missed_items"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TeDGCUmDFb55"
      },
      "outputs": [],
      "source": [
        "def generate_user_embeddings(\n",
        "    actions: pd.DataFrame,\n",
        "    catalog: pd.DataFrame,\n",
        "    text_features: List[str],\n",
        "    cat_features: List[str],\n",
        "    image_column: str,\n",
        "    text_model: object,\n",
        "    image_model: object\n",
        ") -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Генерирует объединенные эмбеддинги пользователей на основе их взаимодействий с товарами.\n",
        "    Комбинирует текстовые, категориальные и визуальные эмбеддинги товаров.\n",
        "\n",
        "    :param actions: DataFrame с историей взаимодействий пользователей\n",
        "    :param catalog: DataFrame с каталогом товаров и их признаками\n",
        "    :param text_features: Список текстовых признаков для генерации эмбеддингов\n",
        "    :param cat_features: Список категориальных признаков для one-hot кодирования\n",
        "    :param image_column: Название столбца с URL изображений\n",
        "    :param text_model: Модель для генерации текстовых эмбеддингов\n",
        "    :param image_model: Модель для генерации визуальных эмбеддингов\n",
        "    :return: np.ndarray (объединенные эмбеддинги пользователей)\n",
        "    \"\"\"\n",
        "    # Загрузка предварительно сохраненных эмбеддингов\n",
        "    item_text_embeddings = pd.read_pickle(HEAD_DIRECTORY+'embeddings/items_names.pkl')\n",
        "    sparse_item_cat_embeddings = sparse_onehot_from_lists(df=catalog, categorical_columns=cat_features)\n",
        "    item_image_embeddings = pd.read_pickle(HEAD_DIRECTORY+'embeddings/items_images.pkl')\n",
        "\n",
        "    # Проверка согласованности данных\n",
        "    assert item_text_embeddings.shape[0] == sparse_item_cat_embeddings.shape[0] == item_image_embeddings.shape[0]\n",
        "\n",
        "    # Создание истории пользователей и mapping'ов\n",
        "    user_histories = create_user_histories(actions)\n",
        "    itemid2index = {catalog['item_id'].iloc[i]: i for i in range(len(catalog['item_id']))}\n",
        "\n",
        "    # Генерация отдельных эмбеддингов\n",
        "    user_text_embeddings, _ = aggregate_user_noncat_embeddings(user_histories, item_text_embeddings, itemid2index)\n",
        "    user_cat_embeddings, userid2index = aggregate_user_cat_embeddings(user_histories, sparse_item_cat_embeddings, itemid2index)\n",
        "    user_image_embeddings, _ = aggregate_user_noncat_embeddings(user_histories, item_image_embeddings, itemid2index)\n",
        "\n",
        "    # Объединение всех эмбеддингов\n",
        "    user_embeddings = np.hstack((user_text_embeddings, user_cat_embeddings, user_image_embeddings))\n",
        "\n",
        "    return user_embeddings, userid2index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MOsKv1Z4GrsJ"
      },
      "outputs": [],
      "source": [
        "def tsne(\n",
        "    vectors: Union[np.ndarray, List[List[float]]],\n",
        "    perplexity: int = 30,\n",
        "    random_state: int = 42\n",
        ") -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Применяет t-SNE для уменьшения размерности векторов до 2D.\n",
        "\n",
        "    Параметры:\n",
        "        vectors: Входные векторы (n_samples, n_features)\n",
        "        perplexity: Параметр perplexity для t-SNE (по умолчанию 30)\n",
        "        random_state: Seed для воспроизводимости (по умолчанию 42)\n",
        "\n",
        "    Возвращает:\n",
        "        Массив формы (n_samples, 2) с проекциями в 2D пространстве\n",
        "    \"\"\"\n",
        "    # Стандартизация данных\n",
        "    scaler = StandardScaler()\n",
        "    scaled_vectors = scaler.fit_transform(vectors)\n",
        "\n",
        "    # Применение t-SNE\n",
        "    tsne = TSNE(n_components=2, perplexity=perplexity, random_state=random_state)\n",
        "    return tsne.fit_transform(scaled_vectors)\n",
        "\n",
        "def visualize_tsne(\n",
        "    projected_vectors: np.ndarray,\n",
        "    title: str = 't-SNE visualization',\n",
        "    figsize: tuple = (10, 8),\n",
        "    point_size: int = 50\n",
        ") -> None:\n",
        "    \"\"\"\n",
        "    Визуализирует 2D проекции векторов.\n",
        "\n",
        "    Параметры:\n",
        "        projected_vectors: Массив проекций формы (n_samples, 2)\n",
        "        title: Заголовок графика (по умолчанию 't-SNE visualization')\n",
        "        figsize: Размер фигуры (по умолчанию (10, 8))\n",
        "        point_size: Размер точек на графике (по умолчанию 50)\n",
        "    \"\"\"\n",
        "    plt.figure(figsize=figsize)\n",
        "    sns.scatterplot(\n",
        "        x=projected_vectors[:, 0],\n",
        "        y=projected_vectors[:, 1],\n",
        "        s=point_size\n",
        "    )\n",
        "\n",
        "    plt.title(title)\n",
        "    plt.xlabel('t-SNE dimension 1')\n",
        "    plt.ylabel('t-SNE dimension 2')\n",
        "    plt.grid(True)\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QEV89n6JY_6p"
      },
      "outputs": [],
      "source": [
        "def dbscan_clustering(data, eps=0.5, min_samples=5, tsne_for_high_dim=True):\n",
        "    \"\"\"\n",
        "    Выполняет кластеризацию DBSCAN и визуализирует результаты.\n",
        "\n",
        "    Параметры:\n",
        "    - data: numpy array (n_samples, n_features)\n",
        "    - eps: максимальное расстояние между соседями для DBSCAN\n",
        "    - min_samples: минимальное количество соседей для core точки\n",
        "    - visualize: если True, строит график кластеров\n",
        "\n",
        "    Возвращает:\n",
        "    - labels: метки кластеров (-1 - выбросы)\n",
        "    \"\"\"\n",
        "    # Стандартизация данных\n",
        "    scaler = StandardScaler()\n",
        "    scaled_data = scaler.fit_transform(data)\n",
        "\n",
        "    # Кластеризация DBSCAN\n",
        "    dbscan = DBSCAN(eps=eps, min_samples=min_samples)\n",
        "    labels = dbscan.fit_predict(scaled_data)\n",
        "\n",
        "    return scaled_data, labels\n",
        "\n",
        "\n",
        "def visualise_dbscan(scaled_data, labels, eps, min_samples):\n",
        "\n",
        "    userid2cluster = {}\n",
        "    for user_id, index in userid2index.items():\n",
        "      userid2cluster[user_id] = labels[index]\n",
        "\n",
        "    plt.figure(figsize=(10, 8))\n",
        "\n",
        "    vis_data = scaled_data[:, :2]\n",
        "    x_label, y_label = 't-SNE dimension 1', 't-SNE dimension 2'\n",
        "\n",
        "    # Визуализация\n",
        "    unique_labels = set(labels)\n",
        "    colors = plt.cm.Spectral(np.linspace(0, 1, len(unique_labels)))\n",
        "\n",
        "    for k, col in zip(unique_labels, colors):\n",
        "        if k == -1:\n",
        "            col = 'gray'  # Выбросы серым\n",
        "\n",
        "        class_member_mask = (labels == k)\n",
        "        xy = vis_data[class_member_mask]\n",
        "        plt.scatter(xy[:, 0], xy[:, 1], c=[col], s=50, label=f'Cluster {k}' if k != -1 else 'Outliers')\n",
        "\n",
        "    plt.title(f'DBSCAN Clustering (eps={eps}, min_samples={min_samples})')\n",
        "    plt.xlabel(x_label)\n",
        "    plt.ylabel(y_label)\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "    return userid2cluster"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GOVYaJ2yKsey"
      },
      "outputs": [],
      "source": [
        "def show_image_from_url(url):\n",
        "    \"\"\"\n",
        "    Проверяет существование изображения по URL и выводит его на экран.\n",
        "\n",
        "    Параметры:\n",
        "        url (str): Ссылка на изображение в интернете.\n",
        "\n",
        "    Возвращает:\n",
        "        bool: True если изображение существует и отображено, иначе False.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Загружаем изображение с проверкой HTTP-статуса\n",
        "        response = requests.get(url, stream=True, timeout=5)\n",
        "        response.raise_for_status()  # Проверяет на ошибки HTTP (404, 403 и т.д.)\n",
        "\n",
        "        # Проверяем, что это действительно изображение\n",
        "        if 'image' not in response.headers['Content-Type']:\n",
        "            print(\"⚠️ URL не ведет к изображению (Content-Type:\", response.headers['Content-Type'], \")\")\n",
        "            return False\n",
        "\n",
        "        # Преобразуем байты в изображение и выводим\n",
        "        img = Image.open(BytesIO(response.content))\n",
        "\n",
        "        plt.figure(figsize=(8, 6))\n",
        "        plt.imshow(img)\n",
        "        plt.axis('off')  # Скрываем оси\n",
        "        plt.title(f\"Изображение из URL:\\n{url[:50]}...\", fontsize=10)\n",
        "        plt.show()\n",
        "        return img\n",
        "\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"❌ Ошибка при загрузке изображения: {e}\")\n",
        "        return False\n",
        "    except Exception as e:\n",
        "        print(f\"⚠️ Неожиданная ошибка: {e}\")\n",
        "        return False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T2d386fSK0Ab"
      },
      "outputs": [],
      "source": [
        "def check_image_url(url):\n",
        "    \"\"\"Проверяет, доступно ли изображение по URL.\"\"\"\n",
        "    try:\n",
        "        response = requests.head(url, timeout=5, allow_redirects=True)\n",
        "        if response.status_code == 200 and 'image' in response.headers.get('Content-Type', ''):\n",
        "            return True\n",
        "        return False\n",
        "    except:\n",
        "        return False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "34ELS4akLP_K"
      },
      "outputs": [],
      "source": [
        "def get_user_histories_by_cluster(\n",
        "    cluster: int,\n",
        "    clusters: Dict[int, int],\n",
        "    interactions_with_features: pd.DataFrame,\n",
        "    n_examples: int = 3\n",
        ") -> None:\n",
        "    \"\"\"\n",
        "    Выводит примеры пользовательских историй для заданного кластера.\n",
        "\n",
        "    Параметры:\n",
        "        cluster: Номер кластера для отображения примеров\n",
        "        clusters: Словарь {user_id: cluster_label} с принадлежностью к кластерам\n",
        "        interactions_with_features: DataFrame с историей взаимодействий пользователей\n",
        "        n_examples: Количество примеров для отображения (по умолчанию 3)\n",
        "\n",
        "    Возвращает:\n",
        "        None (функция только выводит информацию на экран)\n",
        "    \"\"\"\n",
        "    n_printed_examples = 0\n",
        "\n",
        "    for user_id, user_cluster in clusters.items():\n",
        "        if user_cluster == cluster:\n",
        "            display(Markdown(f\"**<font size='+2'>user {user_id} conversions from cluster {user_cluster}</font>**\"))\n",
        "            display_user_actions(\n",
        "                user_id=user_id,\n",
        "                df=interactions_with_features,\n",
        "                n_cols=10,\n",
        "                show_text=True\n",
        "            )\n",
        "            n_printed_examples += 1\n",
        "\n",
        "            if n_printed_examples >= n_examples:\n",
        "                break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uMzazSqtL7dB"
      },
      "outputs": [],
      "source": [
        "def display_user_actions(\n",
        "    user_id: int,\n",
        "    df: pd.DataFrame,\n",
        "    n_cols: int = 5,\n",
        "    cell_width: float = 2.5,\n",
        "    cell_height: float = 3,\n",
        "    show_text: bool = True,\n",
        "    fontsize: int = 8,\n",
        "    max_name_length: int = 20,\n",
        "    timeout: float = 3.0,\n",
        "    image_url_column: str = 'image_url',\n",
        "    item_id_column: str = 'item_id',\n",
        "    name_column: str = 'name'\n",
        ") -> None:\n",
        "    \"\"\"\n",
        "    Визуализирует действия пользователя с фиксированным размером ячеек.\n",
        "\n",
        "    Параметры:\n",
        "        user_id: ID пользователя для отображения\n",
        "        df: DataFrame с данными о взаимодействиях\n",
        "        n_cols: Количество колонок в сетке (по умолчанию 5)\n",
        "        cell_width: Ширина ячейки в дюймах (по умолчанию 2.5)\n",
        "        cell_height: Высота ячейки в дюймах (по умолчанию 3)\n",
        "        show_text: Показывать текстовую информацию (по умолчанию True)\n",
        "        fontsize: Размер шрифта (по умолчанию 8)\n",
        "        max_name_length: Максимальная длина строки названия (по умолчанию 20)\n",
        "        timeout: Таймаут загрузки изображения в секундах (по умолчанию 3.0)\n",
        "        image_url_column: Название колонки с URL изображений (по умолчанию 'image_url')\n",
        "        item_id_column: Название колонки с ID товаров (по умолчанию 'item_id')\n",
        "        name_column: Название колонки с именами товаров (по умолчанию 'name')\n",
        "    \"\"\"\n",
        "    user_items = df[df['user_id'] == user_id]\n",
        "    if user_items.empty:\n",
        "        print(f\"Пользователь {user_id} не найден.\")\n",
        "        return\n",
        "\n",
        "    n_items = len(user_items)\n",
        "    n_rows = int(np.ceil(n_items / n_cols))\n",
        "    figsize = (n_cols * cell_width, n_rows * cell_height)\n",
        "    fig = plt.figure(figsize=figsize)\n",
        "    grid = plt.GridSpec(n_rows, n_cols, wspace=0.1, hspace=0.2)\n",
        "\n",
        "    for idx, (_, row) in enumerate(user_items.iterrows()):\n",
        "        ax = fig.add_subplot(grid[idx // n_cols, idx % n_cols])\n",
        "\n",
        "        try:\n",
        "            # Загрузка и обработка изображения\n",
        "            response = requests.get(row[image_url_column], timeout=timeout)\n",
        "            response.raise_for_status()\n",
        "            img = Image.open(BytesIO(response.content))\n",
        "\n",
        "            # Масштабирование с сохранением пропорций\n",
        "            img_aspect = img.size[0] / img.size[1]\n",
        "            display_width = min(cell_width * 0.9, cell_height * 0.8 * img_aspect)\n",
        "            display_height = display_width / img_aspect\n",
        "\n",
        "            ax.imshow(img, extent=[\n",
        "                (cell_width - display_width)/2,\n",
        "                (cell_width + display_width)/2,\n",
        "                (cell_height - display_height)/2,\n",
        "                (cell_height + display_height)/2\n",
        "            ])\n",
        "\n",
        "            if show_text:\n",
        "                name = '\\n'.join(textwrap.wrap(row[name_column], max_name_length))\n",
        "                ax.text(\n",
        "                    cell_width/2, -0.2*cell_height,\n",
        "                    f\"ID: {row[item_id_column]}\\n{name}\",\n",
        "                    ha='center', va='top',\n",
        "                    fontsize=fontsize, wrap=True\n",
        "                )\n",
        "\n",
        "        except Exception as e:\n",
        "            # Обработка ошибок загрузки изображения\n",
        "            ax.add_patch(plt.Rectangle(\n",
        "                (0, 0), cell_width, cell_height,\n",
        "                color='lightgray'\n",
        "            ))\n",
        "            if show_text:\n",
        "                ax.text(\n",
        "                    cell_width/2, cell_height/2, 'X',\n",
        "                    color='red', ha='center', va='center'\n",
        "                )\n",
        "                ax.text(\n",
        "                    cell_width/2, -0.2*cell_height,\n",
        "                    f\"ID: {row[item_id_column]}\",\n",
        "                    ha='center', va='top', fontsize=fontsize\n",
        "                )\n",
        "\n",
        "        ax.set_xlim(0, cell_width)\n",
        "        ax.set_ylim(-0.3*cell_height if show_text else 0, cell_height)\n",
        "        ax.axis('off')\n",
        "\n",
        "    # Скрытие пустых ячеек\n",
        "    for idx in range(n_items, n_rows * n_cols):\n",
        "        fig.add_subplot(grid[idx // n_cols, idx % n_cols]).axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Umd6xzxeN7HY"
      },
      "outputs": [],
      "source": [
        "class PopularInUserModel():\n",
        "    def __init__(self, k: int = 10):\n",
        "        \"\"\"\n",
        "        Рекомендательная система на основе популярности.\n",
        "        :param top_n: Количество рекомендаций на пользователя\n",
        "        \"\"\"\n",
        "        self.top_n = k\n",
        "        self.item_popularity = None\n",
        "        self.user_top_items = None\n",
        "\n",
        "    def fit(self, interactions: pd.DataFrame):\n",
        "        \"\"\"\n",
        "        Обучает модель на данных о взаимодействиях пользователей с товарами.\n",
        "        :param interactions: DataFrame с колонками ['user_id', 'item_id']\n",
        "        \"\"\"\n",
        "        # Подсчет популярности товаров\n",
        "        item_counts = interactions['item_id'].value_counts().reset_index()\n",
        "        item_counts.columns = ['item_id', 'score']\n",
        "        self.item_popularity = item_counts\n",
        "\n",
        "        # Топ-10 популярных товаров для каждого пользователя\n",
        "        user_top_items = (\n",
        "            interactions.groupby('user_id')['item_id']\n",
        "            .value_counts()\n",
        "            .reset_index(name='score')\n",
        "        )\n",
        "\n",
        "        self.user_top_items = user_top_items[user_top_items.groupby('user_id').cumcount() < self.top_n]\n",
        "\n",
        "    def recommend(self, user_ids: List[int], train_dataset = None, k = None, filter_viewed = None) -> pd.DataFrame:\n",
        "        \"\"\"\n",
        "        Возвращает DataFrame с рекомендациями в формате user_id, item_id, score, rank.\n",
        "        :param user_ids: Список ID пользователей\n",
        "        :return: DataFrame с рекомендациями\n",
        "        \"\"\"\n",
        "        if self.user_top_items is None or self.item_popularity is None:\n",
        "            raise ValueError(\"Модель не обучена. Вызовите .fit() перед рекомендациями.\")\n",
        "\n",
        "        recommendations = []\n",
        "        for user_id in user_ids:\n",
        "            user_items = self.user_top_items[self.user_top_items['user_id'] == user_id].sort_values(by='score', ascending=False)\n",
        "\n",
        "            # Дополняем недостающие рекомендации самыми популярными товарами\n",
        "            if len(user_items) < self.top_n:\n",
        "                additional_items = self.item_popularity[~self.item_popularity['item_id'].isin(user_items['item_id'])]\n",
        "                additional_items = additional_items.head(self.top_n - len(user_items)).copy().sort_values(by='score', ascending=False)\n",
        "                additional_items['user_id'] = user_id\n",
        "                user_items = pd.concat([user_items, additional_items], ignore_index=True)\n",
        "\n",
        "            user_items = user_items.head(self.top_n)\n",
        "            user_items['rank'] = range(1, len(user_items) + 1)\n",
        "            recommendations.append(user_items)\n",
        "\n",
        "        return pd.concat(recommendations, ignore_index=True)[['user_id', 'item_id', 'score', 'rank']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yeAgRbDjOC6E"
      },
      "outputs": [],
      "source": [
        "def keep_top_10_clusters(labels):\n",
        "    \"\"\"\n",
        "    Оставляет только 10 самых многочисленных кластеров, остальные помечает как шум (-1).\n",
        "\n",
        "    Параметры:\n",
        "    - labels: массив с метками кластеров (например, результат DBSCAN)\n",
        "\n",
        "    Возвращает:\n",
        "    - Модифицированный массив меток, где только 10 самых больших кластеров сохранили свои номера,\n",
        "      а остальным присвоено -1\n",
        "    \"\"\"\n",
        "    # Считаем количество точек в каждом кластере (исключая шум -1)\n",
        "    cluster_counts = Counter(labels)\n",
        "    if -1 in cluster_counts:\n",
        "        del cluster_counts[-1]  # Игнорируем точки шума\n",
        "\n",
        "    # Если кластеров 10 или меньше - ничего не меняем\n",
        "    if len(cluster_counts) <= 10:\n",
        "        return labels.copy()\n",
        "\n",
        "    # Находим 10 самых больших кластеров\n",
        "    top_10_clusters = [cluster for cluster, _ in cluster_counts.most_common(10)]\n",
        "\n",
        "    # Создаем новый массив меток\n",
        "    new_labels = np.full_like(labels, -1)  # По умолчанию - шум\n",
        "\n",
        "    # Сохраняем только топ-10 кластеров\n",
        "    for cluster in top_10_clusters:\n",
        "        new_labels[labels == cluster] = cluster\n",
        "\n",
        "    return new_labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hPLStMa2OJBy"
      },
      "outputs": [],
      "source": [
        "def get_cluster_examples(userid2clusters: Dict[int, int]):\n",
        "  cluster_examples = {}\n",
        "  for cluster in np.unique(list(userid2clusters.values()))[1:]:\n",
        "    for user_id, user_cluster in userid2clusters.items():\n",
        "      if user_cluster == cluster:\n",
        "        cluster_examples[cluster] = user_id\n",
        "        break\n",
        "\n",
        "  return cluster_examples"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sLbB0JK9Im31"
      },
      "source": [
        "# Данные\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ux9A7EW0JDgQ"
      },
      "outputs": [],
      "source": [
        "# sasrec_data_folder_path = HEAD_DIRECTORY+'data/sasrec_format/'\n",
        "# output_name = 'actions'\n",
        "# data_actions_processed = spark.read.parquet(sasrec_data_folder_path+output_name).orderBy(['user_id', 'datetime'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NkSsC4U2I7qD"
      },
      "outputs": [],
      "source": [
        "# sasrec_data_folder_path = HEAD_DIRECTORY+'data/sasrec_format/'\n",
        "# output_name = 'items'\n",
        "# data_items_processed = spark.read.parquet(sasrec_data_folder_path+output_name).orderBy('id', 'feature', 'value')\n",
        "# data_items_cleaned = spark.read.parquet(HEAD_DIRECTORY+'data/cleaned_data/data_items')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oPuu7eorIxt6"
      },
      "outputs": [],
      "source": [
        "# actions, items_features = process_data.processed_datasets_to_pandas(data_actions_processed, data_items_processed)\n",
        "# actions.to_pickle(HEAD_DIRECTORY+'data/sasrec_format/actions.pkl')\n",
        "# items_features.to_pickle(HEAD_DIRECTORY+'data/sasrec_format/items.pkl')\n",
        "actions = pd.read_pickle(HEAD_DIRECTORY+'data/sasrec_format/actions.pkl')\n",
        "items = pd.read_pickle(HEAD_DIRECTORY+'data/sasrec_format/items.pkl')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h6MS3mjcIrYu"
      },
      "outputs": [],
      "source": [
        "# catalog = data_items_cleaned.select('customer_id', 'name', 'group_ids_intersect', 'How_to_get_it', 'Primary_Brand', 'Primary_Pet_Type', 'image_url').toPandas()\n",
        "# catalog.rename(columns={'customer_id': 'item_id'}, inplace=True)\n",
        "# catalog['item_id'] = catalog['item_id'].apply(lambda x: int(x))\n",
        "# catalog = catalog.rename(columns={'group_ids_intersect': 'Category', 'How_to_get_it': 'Delivery', 'Primary_Brand': 'Brand', 'Primary_Pet_Type': 'Pet'})\n",
        "# catalog['Pet'] = catalog['Pet'].apply(lambda x: x[0].split(',') if x else x)\n",
        "# catalog.to_pickle(HEAD_DIRECTORY+'data/sasrec_format/catalog.pkl')\n",
        "catalog = pd.read_pickle(HEAD_DIRECTORY+'data/sasrec_format/catalog.pkl')\n",
        "actions_with_item_features = actions.merge(catalog, on='item_id', how='left')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ePYFvRGDTwQ8"
      },
      "outputs": [],
      "source": [
        "# В валлидационный датасет откладываем последнее действие для каждого юзера\n",
        "val_df = actions.loc[actions.groupby(\"user_id\")[\"datetime\"].idxmax()]\n",
        "\n",
        "# Обучающий датасет – все, кроме последней итерации у каждого пользователя\n",
        "train_df = actions.drop(val_df.index)\n",
        "train_df_with_features = train_df.merge(catalog, on='item_id', how='left')"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9kTaj7TKkKQH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}